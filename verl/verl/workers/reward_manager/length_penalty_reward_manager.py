# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections import defaultdict
import torch
import numpy as np
import logging
import requests
import re
import time
from verl import DataProto
from verl.utils.reward_score import default_compute_score
from verl.workers.reward_manager import register
from verl.workers.reward_manager.abstract import AbstractRewardManager
from .async_openai_worker import AsyncOpenAIManager, get_request_hash
import asyncio
from concurrent.futures import ThreadPoolExecutor
import threading

logger = logging.getLogger(__name__)


@register("length_penalty")
class LengthPenaltyRewardManager(AbstractRewardManager):
    """
    A reward manager that applies a penalty based on the length of the response.
    The penalty is proportional to the distance from the median response length for a given prompt.
    """

    def __init__(
        self,
        tokenizer,
        num_examine,
        compute_score=None,
        reward_fn_key="data_source",
        length_penalty_config=None,
    ) -> None:
        self.tokenizer = tokenizer
        self.num_examine = num_examine
        self.compute_score = compute_score or default_compute_score
        self.reward_fn_key = reward_fn_key
        self.length_penalty_config = length_penalty_config

        # Extract OpenAI configs from length_penalty_config (MUST be first)
        self.openai_api_key = getattr(self.length_penalty_config, "api_key", None)
        self.openai_model_name = getattr(self.length_penalty_config, "model_name", "deepseek-v3")
        self.openai_reward_coefficient = getattr(self.length_penalty_config, "reward_coefficient", 1.0)
        self.openai_api_endpoint = getattr(self.length_penalty_config, "api_endpoint", "https://qianfan.baidubce.com/v2/chat/completions")
        self.openai_system_prompt = '''
# è§’è‰²
ä½ æ˜¯ä¸€ä¸ªé«˜åº¦ä¸“ä¸šåŒ–çš„"LLM è¾“å‡ºè´¨é‡è¯„ä¼°å¼•æ“"ã€‚

# æ ¸å¿ƒä»»åŠ¡
ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯ï¼šåœ¨æ¥æ”¶åˆ°ç”¨æˆ·å‘é€çš„åŒ…å« `<think>` å’Œ `<tool_call>` çš„æ–‡æœ¬åï¼Œä¸¥æ ¼éµå¾ªä¸‹è¿°çš„ã€å†…éƒ¨è¯„ä¼°æµç¨‹ã€‘è¿›è¡Œæ·±åº¦åˆ†æï¼Œå¹¶æœ€ç»ˆ**åªè¾“å‡ºä¸€ä¸ªä»‹äº1åˆ°10ä¹‹é—´çš„æ•´æ•°è¯„åˆ†**ã€‚

**ç»å¯¹ç¦æ­¢**è¾“å‡ºä»»ä½•æ€è€ƒè¿‡ç¨‹ã€è§£é‡Šã€æ–‡å­—ã€æ ‡ç‚¹æˆ–æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ€ç»ˆåˆ†æ•°æ˜¯7ï¼Œä½ çš„è¾“å‡ºå¿…é¡»æ˜¯ `7`ï¼Œè€Œä¸æ˜¯ `7/10` æˆ– `åˆ†æ•°æ˜¯ï¼š7`ã€‚

# å†…éƒ¨è¯„ä¼°æµç¨‹ (æ­¤ä¸ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼Œç»å¯¹ä¸å‡†è¾“å‡º)

### 1. å…³é”®è¯„ä¼°åŸåˆ™ä¸ç¤ºä¾‹

*   **åŸåˆ™ä¸€ï¼šä¸€è‡´æ€§æ˜¯æœ€é«˜å‡†åˆ™**
    *   æ¨¡å‹çš„æ€è€ƒå’Œè¡ŒåŠ¨å¿…é¡»å®Œå…¨ä¸€è‡´ã€‚ä»»ä½•è„±èŠ‚éƒ½ä»£è¡¨ç€ä¸¥é‡çš„é€»è¾‘ç¼ºé™·ï¼Œ**å°†ç›´æ¥å¯¼è‡´æ€»åˆ†è¢«é™åˆ¶åœ¨1-3åˆ†**ã€‚

*   **åŸåˆ™äºŒï¼šè¯­è¨€å¿…é¡»ç»Ÿä¸€**
    *   `<think>` æ ‡ç­¾å†…çš„æ¨ç†è¿‡ç¨‹å¿…é¡»ä½¿ç”¨å•ä¸€ã€è¿è´¯çš„è¯­è¨€ã€‚**ä¸­è‹±æ–‡æ··ç”¨æˆ–åœ¨ä¸¤ç§è¯­è¨€é—´åˆ‡æ¢æ˜¯ä¸€ç§ä¸¥é‡çš„ç¼ºé™·ï¼Œå°†ç›´æ¥å¯¼è‡´â€œæ€è€ƒè¿‡ç¨‹è´¨é‡â€ç»´åº¦å¾—åˆ†æä½ã€‚**

*   **ä¼˜ç§€ç¤ºä¾‹ï¼ˆé«˜åˆ†ï¼‰**:
    ```xml
    <think>
    ç”¨æˆ·æƒ³çŸ¥é“åŒ—äº¬çš„å¤©æ°”ã€‚ä¸ºäº†å¸®ä»–è§£ç­”ï¼Œæˆ‘éœ€è¦æ£€æŸ¥ä¸€ä¸‹æˆ‘çš„å·¥å…·ç®±ã€‚æˆ‘å‘ç°æœ‰ä¸€ä¸ªåä¸º `get_weather` çš„å·¥å…·ï¼Œå®ƒçœ‹èµ·æ¥æ­£å¥½èƒ½ç”¨ã€‚è¿™ä¸ªå·¥å…·éœ€è¦ä¸€ä¸ª `city` å‚æ•°ï¼Œç”¨æˆ·åœ¨æé—®ä¸­å·²ç»æ˜ç¡®æåˆ°äº†â€œåŒ—äº¬â€ã€‚å› æ­¤ï¼Œæœ€åˆç†çš„ä¸‹ä¸€æ­¥å°±æ˜¯è°ƒç”¨ `get_weather` å·¥å…·ï¼Œå¹¶æŠŠâ€œåŒ—äº¬â€ä½œä¸ºåŸå¸‚å‚æ•°ä¼ ç»™å®ƒã€‚
    </think>
    <tool_call>
    {"name": "get_weather", "arguments": {"city":"åŒ—äº¬"}}
    </tool_call>
    ```

*   **ä¸å¯æ¥å—çš„ä¾‹å­ï¼ˆä½åˆ†ï¼‰**:
    ```xml
    <think>
    ç”¨æˆ·æƒ³è®¢ä¸€å¼ ä»ä¸Šæµ·åˆ°åŒ—äº¬çš„æœºç¥¨ã€‚å¥½çš„ï¼Œæˆ‘åº”è¯¥ä½¿ç”¨ `book_flight` å·¥å…·ã€‚æˆ‘éœ€è¦å‡ºå‘åœ°å’Œç›®çš„åœ°ã€‚ç”¨æˆ·çš„æŒ‡ä»¤å¾ˆæ˜ç¡®ï¼Œå‡ºå‘åœ°æ˜¯'ä¸Šæµ·'ï¼Œç›®çš„åœ°æ˜¯'åŒ—äº¬'ã€‚
    </think>
    <tool_call>
    {"name": "book_flight", "arguments": {"departure_city": "åŒ—äº¬", "destination_city": "ä¸Šæµ·"}}
    </tool_call>
    ```

### 2. æ ¸å¿ƒè¯„ä¼°ç»´åº¦

ä½ éœ€è¦åœ¨å†…å¿ƒä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦è¿›è¡Œæ‰“åˆ†ï¼Œå¹¶æœ€ç»ˆåŠ æƒå¾—å‡ºæ€»åˆ†ã€‚

*   **A. æ€è€ƒ-è¡ŒåŠ¨ä¸€è‡´æ€§ (æƒé‡: 30%)**:
    *   æ£€æŸ¥ `<tool_call>` çš„å‡½æ•°åå’Œå‚æ•°æ˜¯å¦æ˜¯ `<think>` è¿‡ç¨‹çš„ç›´æ¥ã€åˆä¹é€»è¾‘çš„ç»“è®ºã€‚
    *   **å†…å¿ƒè¯„åˆ†**: 1-10åˆ†ã€‚

*   **B. æ€è€ƒè¿‡ç¨‹çš„è´¨é‡ä¸æ¸…æ™°åº¦ (æƒé‡: 60%)**:
    *   **é€»è¾‘æ€§**: æ˜¯å¦æ­£ç¡®ç†è§£ç”¨æˆ·æ„å›¾ï¼Ÿæ¨ç†æ­¥éª¤æ˜¯å¦è¿è´¯ã€åˆç†ï¼Œå¹¶ä¸”ç›´æŒ‡æœ€ç»ˆçš„å·¥å…·è°ƒç”¨ï¼Ÿ
    *   **æ¨ç†é£æ ¼ä¸è´¨é‡**: æ¨ç†è¿‡ç¨‹åº”åƒä¸€ä¸ªé¢†åŸŸä¸“å®¶è§£å†³é—®é¢˜æ—¶çš„å†…å¿ƒç‹¬ç™½ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªç¨‹åºåœ¨æ‰“å°è°ƒè¯•æ—¥å¿—ã€‚åŸºäºæ­¤ï¼Œå¯¹ä»¥ä¸‹è¡Œä¸ºè¿›è¡Œ**ä¸¥å‰æƒ©ç½š**ï¼š
        *   **ç¦æ­¢å…ƒè®¤çŸ¥æè¿° (Meta-Commentary)**: æ€è€ƒè¿‡ç¨‹åº”ä¸“æ³¨äº **â€œåšä»€ä¹ˆâ€** å’Œ **â€œä¸ºä»€ä¹ˆåšâ€**ï¼Œè€Œä¸æ˜¯æè¿°å…¶è‡ªèº«çš„æ€è€ƒæ­¥éª¤ã€‚ä¸¥å‰æƒ©ç½šä»»ä½•å‡ºç°â€œå“åº”è§„åˆ™â€ã€â€œå‚æ•°è®¾ç½®â€ã€â€œæœ€ç»ˆå“åº”â€ã€â€œç¡®è®¤å‡½æ•°è°ƒç”¨â€ã€â€œå“åº”æ ¼å¼â€ç­‰æè¿°ç”Ÿæˆè¿‡ç¨‹çš„è¯è¯­ã€‚
        *   **ç¦æ­¢æ¨¡æ¿åŒ–ä¸å†—ä½™**: æ¨ç†åº”è‡ªç„¶ã€ç›´æˆªäº†å½“ã€‚ä¸¥å‰æƒ©ç½šä½¿ç”¨â€œå›é¡¾å·¥å…·æè¿°â€ã€â€œæ£€æŸ¥è°ƒç”¨è§„èŒƒâ€ç­‰æœºæ¢°çŸ­è¯­ï¼Œä»¥åŠå¯¹åŒä¸€ç»“è®ºçš„åå¤ç¡®è®¤ã€‚**å°¤å…¶ç¦æ­¢åœ¨æ€è€ƒçš„ç»“å°¾å¤„å¤è¿°æœ€ç»ˆçš„`tool_call`å†…å®¹ã€‚**
    *   **å®Œæ•´æ€§ä¸æ­£ç¡®æ€§**: æ€è€ƒè¿‡ç¨‹çš„æ–‡æœ¬å¿…é¡»æ˜¯å®Œæ•´çš„å¥å­ï¼Œ**æ²¡æœ‰ä¸­é€”æˆªæ–­**ã€‚**ä¸å¾—åŒ…å«ä»»ä½•æ‹¼å†™é”™è¯¯æˆ–æ˜æ˜¾çš„è¯­æ³•é”™è¯¯**ã€‚
    *   **è¯­è¨€çº¯ç²¹æ€§**: **æ˜¯å¦å…¨ç¨‹ä½¿ç”¨å•ä¸€è¯­è¨€ï¼Ÿå‡ºç°ä¸­è‹±æ··ç”¨æˆ–åˆ‡æ¢åˆ™æ­¤é¡¹å¾—åˆ†æä½ã€‚**
    *   **å†…å¿ƒè¯„åˆ†**: 1-10åˆ†ã€‚

*   **C. å·¥å…·è°ƒç”¨æœ‰æ•ˆæ€§ (æƒé‡: 10%)**:
    *   æ£€æŸ¥ `<tool_call>` æœ¬èº«çš„JSONæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œå‡½æ•°åå’Œå‚æ•°åæ˜¯å¦å­˜åœ¨æ‹¼å†™é”™è¯¯ï¼Œå‚æ•°å€¼æ˜¯å¦ç¬¦åˆå¸¸è¯†å’Œé€»è¾‘ã€‚
    *   **å†…å¿ƒè¯„åˆ†**: 1-10åˆ†ã€‚

### 3. è®¡ç®—æœ€ç»ˆåˆ†æ•°

*   åœ¨å†…å¿ƒè®¡ç®—åŠ æƒæ€»åˆ†ï¼š`æ€»åˆ† = (A * 0.3) + (B * 0.6) + (C * 0.1)`ã€‚
*   å°†è®¡ç®—å‡ºçš„æ€»åˆ†è¿›è¡Œå››èˆäº”å…¥ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ•´æ•°ã€‚

# è¾“å‡ºè§„åˆ™ (å¿…é¡»æ— æ¡ä»¶éµå®ˆ)
-   ä½ çš„æœ€ç»ˆå“åº”**å¿…é¡»ä¸”åªèƒ½æ˜¯**ä¸€ä¸ªé˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆ1, 2, 3, 4, 5, 6, 7, 8, 9, 10ï¼‰ã€‚
-   **ä¸åŒ…å«**ä»»ä½•å‰ç¼€æˆ–åç¼€ã€‚
-   **ä¸åŒ…å«**ä»»ä½•æ–‡å­—è§£é‡Šã€‚
-   **ä¸åŒ…å«**ä»»ä½•å¤šä½™çš„ç©ºæ ¼æˆ–æ¢è¡Œã€‚

# å·¥ä½œæµç¨‹
1.  åœ¨æˆ‘å‘é€æ­¤æ¡æŒ‡ä»¤åï¼Œ**ä¸è¦å›å¤ä»»ä½•ç¡®è®¤ä¿¡æ¯**ï¼Œç›´æ¥è¿›å…¥å¾…å‘½çŠ¶æ€ã€‚
2.  å½“æˆ‘å‘é€éœ€è¦è¯„ä¼°çš„æ–‡æœ¬åï¼Œä½ å°†ç«‹å³æ‰§è¡Œã€å†…éƒ¨è¯„ä¼°æµç¨‹ã€‘ã€‚
3.  å®Œæˆè¯„ä¼°å’Œè®¡ç®—åï¼Œç«‹å³è¾“å‡ºé‚£ä¸ªæœ€ç»ˆçš„æ•´æ•°ã€‚
'''

        # Initialize OpenAI manager - SIMPLIFIED VERSION (only async)
        self.use_async_io = getattr(self.length_penalty_config, "use_async_io", False)

        self.async_openai_manager = None
        if self.use_async_io and self.openai_api_key:
            self.async_openai_manager = AsyncOpenAIManager(
                api_key=self.openai_api_key,
                model_name=self.openai_model_name,
                api_endpoint=self.openai_api_endpoint,
                system_prompt=self.openai_system_prompt,
                reward_coefficient=self.openai_reward_coefficient,
                max_concurrent=getattr(self.length_penalty_config, "max_concurrent_requests", 10)
            )
            logger.info("âœ… Initialized async OpenAI manager for TRUE ASYNC I/O (zero GPU wait)")
        elif self.openai_api_key:
            logger.info("â„¹ï¸  OpenAI API key provided but use_async_io=False. Using synchronous fallback.")

        # Event loop for async operations
        self._event_loop = None
        self._executor = None

    def start_workers(self):
        """å¯åŠ¨å¼‚æ­¥ OpenAI manager"""
        if self.async_openai_manager:
            logger.info("Async OpenAI manager is initialized and ready (no separate start needed)")
        else:
            logger.info("No async OpenAI manager to start (use_async_io=True to enable)")

    def shutdown_workers(self):
        """å…³é—­å¼‚æ­¥ OpenAI ç®¡ç†å™¨"""
        # å…³é—­å¼‚æ­¥ OpenAI manager
        if self.async_openai_manager:
            logger.info("Shutting down async OpenAI manager...")
            if self._event_loop and self._event_loop.is_running():
                # åœ¨äº‹ä»¶å¾ªç¯ä¸­å…³é—­å¼‚æ­¥ç®¡ç†å™¨
                future = asyncio.run_coroutine_threadsafe(
                    self.async_openai_manager.shutdown(),
                    self._event_loop
                )
                try:
                    future.result(timeout=5.0)
                except Exception as e:
                    logger.error(f"Error shutting down async manager: {e}")

            # å…³é—­äº‹ä»¶å¾ªç¯å’Œçº¿ç¨‹æ± 
            if self._executor:
                self._executor.shutdown(wait=True)
                self._executor = None
            if self._event_loop and not self._event_loop.is_closed():
                self._event_loop.close()
                self._event_loop = None

            logger.info("Async OpenAI manager shut down successfully.")
        else:
            logger.info("No OpenAI manager to shut down (not initialized)")

    def _ensure_event_loop(self):
        """ç¡®ä¿äº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œ"""
        if self._event_loop is None or self._event_loop.is_closed():
            logger.info("Starting new event loop for async operations...")
            self._executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix="async-openai")
            self._event_loop = asyncio.new_event_loop()

            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­å¯åŠ¨äº‹ä»¶å¾ªç¯
            def run_event_loop(loop):
                asyncio.set_event_loop(loop)
                loop.run_forever()

            self._event_loop_thread = threading.Thread(
                target=run_event_loop,
                args=(self._event_loop,),
                daemon=True
            )
            self._event_loop_thread.start()
            logger.info("Event loop started in background thread")

    def _get_single_openai_quality_reward(self, response_str: str, response_format_reward: float) -> float:
        if response_format_reward == 0:
            return 0.0

        if not self.openai_api_key:
            logger.warning("OpenAI API key not provided. Skipping OpenAI quality evaluation.")
            return 0.0

        headers = {
            "Authorization": f"Bearer {self.openai_api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": self.openai_model_name,
            "messages": [
                {"role": "system", "content": self.openai_system_prompt},
                {"role": "user", "content": response_str},
            ],
            "temperature": 0.0, # For consistent evaluation
        }

        try:
            response = requests.post(self.openai_api_endpoint, headers=headers, json=payload)
            response.raise_for_status() # Raise an exception for HTTP errors
            response_json = response.json()
            
            # Extract content from the response
            model_output = response_json["choices"][0]["message"]["content"]
            
            # Parse the score using regex
            match = re.search(r'æœ€ç»ˆè¯„åˆ†\s+(\d+)', model_output)
            if match:
                score = float(match.group(1))
                return score * self.openai_reward_coefficient
            else:
                logger.warning(f"Could not parse OpenAI score from: {model_output}")
                return 0.0
        except requests.exceptions.RequestException as e:
            logger.error(f"OpenAI API request failed: {e}")
            return 0.0
        except (KeyError, IndexError) as e:
            logger.error(f"Error parsing OpenAI API response: {e}\nResponse: {response_json}")
            return 0.0

    def _get_batched_openai_quality_rewards_sync(self, responses_to_evaluate: list[tuple[str, float]]) -> list[float]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„ OpenAI è´¨é‡è¯„ä¼°ï¼Œä½œä¸ºå¤‡é€‰æ–¹æ¡ˆã€‚
        """
        if not self.openai_api_key:
            logger.warning("OpenAI API key not provided. Skipping OpenAI quality evaluation.")
            return [0.0] * len(responses_to_evaluate)

        import concurrent.futures

        max_workers = 5
        scores = [0.0] * len(responses_to_evaluate)

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_index = {
                executor.submit(self._get_single_openai_quality_reward, resp_str, resp_format_reward): i
                for i, (resp_str, resp_format_reward) in enumerate(responses_to_evaluate)
            }

            for future in concurrent.futures.as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    score = future.result()
                    scores[index] = score
                except Exception as exc:
                    logger.error(f"OpenAI quality evaluation generated an exception for response at index {index}: {exc}")
                    scores[index] = 0.0

        return scores

    def _get_batched_openai_quality_rewards_async(self, responses_to_evaluate: list[tuple[str, float]]) -> list[float]:
        """
        çœŸæ­£çš„å¼‚æ­¥å¹¶è¡Œ OpenAI è´¨é‡è¯„ä¼° - é›¶ GPU ç­‰å¾…æ—¶é—´

        è¿™ä¸ªæ–¹æ³•çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š
        1. ç«‹å³è¿”å› Future å¯¹è±¡ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
        2. GPU å¯ä»¥ç»§ç»­è¿›è¡Œå…¶ä»–è®¡ç®—
        3. API è¯·æ±‚åœ¨åå°å¼‚æ­¥å¹¶å‘æ‰§è¡Œ
        4. åªåœ¨å¿…è¦æ—¶æ£€æŸ¥ç»“æœï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡
        """
        if not self.async_openai_manager:
            logger.warning("Async OpenAI manager not available")
            return [0.0] * len(responses_to_evaluate)

        if not self.openai_api_key:
            logger.warning("OpenAI API key not provided. Skipping OpenAI quality evaluation.")
            return [0.0] * len(responses_to_evaluate)

        # ç¡®ä¿äº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œ
        self._ensure_event_loop()

        # æäº¤æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡
        async def submit_and_evaluate():
            results = await self.async_openai_manager.submit_and_get_batch_results(
                responses_to_evaluate,
                check_interval=0.001  # æœ€å°æ£€æŸ¥é—´éš”
            )
            return results

        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå¼‚æ­¥æ“ä½œ
        future = asyncio.run_coroutine_threadsafe(submit_and_evaluate(), self._event_loop)

        # ç­‰å¾…ç»“æœï¼Œä½†ä¸»çº¿ç¨‹å¯ä»¥åšå…¶ä»–äº‹æƒ…
        try:
            # è®¾ç½®è¶…æ—¶ä½†å¯ä»¥è°ƒæ•´
            timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶
            results = future.result(timeout=timeout)
            logger.info(f"Completed async OpenAI evaluation for {len(responses_to_evaluate)} responses")
            return results
        except asyncio.TimeoutError:
            logger.error(f"Async OpenAI evaluation timed out after {timeout} seconds")
            return [0.0] * len(responses_to_evaluate)
        except Exception as e:
            logger.error(f"Async OpenAI evaluation failed: {e}")
            return [0.0] * len(responses_to_evaluate)

    def _get_batched_openai_quality_rewards_non_blocking(
        self,
        responses_to_evaluate: list[tuple[str, float]]
    ) -> tuple[list[float], asyncio.Future]:
        """
        éé˜»å¡ç‰ˆæœ¬çš„å¼‚æ­¥è¯„ä¼° - è¿”å› Future å’Œåˆå§‹ç»“æœ

        è¿™ä¸ªæ–¹æ³•æ˜¯å…³é”®ä¼˜åŒ–ï¼š
        1. ç«‹å³è¿”å›åˆå§‹ç»“æœï¼ˆ0.0ï¼‰
        2. è¿”å› Future å¯¹è±¡ä¾›åç»­æ£€æŸ¥
        3. ä¸»çº¿ç¨‹å¯ä»¥ç«‹å³ç»§ç»­ GPU è®¡ç®—
        4. åœ¨è®¡ç®—é—´éš™å¼‚æ­¥æ£€æŸ¥ API ç»“æœ

        Returns:
            (initial_results, future) - åˆå§‹ç»“æœå’Œå¼‚æ­¥Future
        """
        if not self.async_openai_manager or not self.openai_api_key:
            # å¦‚æœæ²¡æœ‰å¼‚æ­¥ç®¡ç†å™¨ï¼Œè¿”å›é›¶ç»“æœå’Œç©ºçš„Future
            initial_results = [0.0] * len(responses_to_evaluate)
            dummy_future = asyncio.Future()
            dummy_future.set_result(initial_results)
            return initial_results, dummy_future

        # ç¡®ä¿äº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œ
        self._ensure_event_loop()

        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        async def evaluate_async():
            try:
                results = await self.async_openai_manager.submit_and_get_batch_results(
                    responses_to_evaluate,
                    check_interval=0.001
                )
                return results
            except Exception as e:
                logger.error(f"Async evaluation failed: {e}")
                return [0.0] * len(responses_to_evaluate)

        # æäº¤å¼‚æ­¥ä»»åŠ¡
        future = asyncio.run_coroutine_threadsafe(evaluate_async(), self._event_loop)

        # ç«‹å³è¿”å›åˆå§‹ç»“æœï¼Œä¸»çº¿ç¨‹å¯ä»¥ç»§ç»­
        initial_results = [
            0.0 if format_reward > 0 else 0.0
            for _, format_reward in responses_to_evaluate
        ]

        return initial_results, future

    def __call__(self, data: DataProto, return_dict: bool = False):
        """
        è®¡ç®—å¥–åŠ±åˆ†æ•°ï¼Œå®ç° GPU è®¡ç®—ä¸ OpenAI API è¯·æ±‚çš„è§£è€¦ã€‚
        åœ¨å¤„ç†æ¯ä¸ª batch æ—¶ï¼Œå¼‚æ­¥æäº¤ OpenAI è¯„ä¼°è¯·æ±‚ï¼Œè®© GPU ç»§ç»­å¤„ç†å…¶ä»–è®¡ç®—ã€‚
        """
        # ç¡®ä¿å¼‚æ­¥ç®¡ç†å™¨å·²åˆå§‹åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.async_openai_manager:
            if not hasattr(self, '_workers_started') or not self._workers_started:
                self.start_workers()
                self._workers_started = True

        if "rm_scores" in data.batch.keys():
            if return_dict:
                reward_extra_keys = data.meta_info.get("reward_extra_keys", [])
                reward_extra_info = {key: data.non_tensor_batch[key] for key in reward_extra_keys}
                return {"reward_tensor": data.batch["rm_scores"], "reward_extra_info": reward_extra_info}
            else:
                return data.batch["rm_scores"]

        reward_tensor = torch.zeros_like(data.batch["responses"], dtype=torch.float32)
        reward_extra_info = defaultdict(list)
        
        # Group responses by prompt uid
        prompt_groups = defaultdict(list)
        uids = data.non_tensor_batch.get("uid", list(range(len(data))))
        assert len(uids) == len(data), f"UID list length ({len(uids)}) does not match batch size ({len(data)})."
        for i, uid in enumerate(uids):
            prompt_groups[uid].append(i)
        
        print(f"[PENALTY DEBUG] Total groups identified: {len(prompt_groups)}")

        # Calculate length penalty for each group
        for _, indices in prompt_groups.items():
            print(f"[PENALTY DEBUG] Processing group with indices {indices}")
            
            # Store data needed for each item in the group
            group_data = []
            for i in indices:
                data_item = data[i]
                
                prompt_ids = data_item.batch["prompts"]
                prompt_length = prompt_ids.shape[-1]
                valid_prompt_length = data_item.batch["attention_mask"][:prompt_length].sum()
                valid_prompt_ids = prompt_ids[-valid_prompt_length:]
                response_ids = data_item.batch["responses"]
                valid_response_length = data_item.batch["attention_mask"][prompt_length:].sum()
                valid_response_ids = response_ids[:valid_response_length]
                
                prompt_str = self.tokenizer.decode(valid_prompt_ids, skip_special_tokens=True)
                response_str = self.tokenizer.decode(valid_response_ids, skip_special_tokens=True)
                eos_token = self.tokenizer.eos_token
                if response_str.endswith(eos_token):
                    response_str = response_str[: -len(eos_token)]

                ground_truth = data_item.non_tensor_batch["reward_model"]["ground_truth"]
                data_source = data_item.non_tensor_batch[self.reward_fn_key]
                extra_info = data_item.non_tensor_batch.get("extra_info", {})
                
                result = self.compute_score(
                    data_source=data_source,
                    solution_str=response_str,
                    ground_truth=ground_truth,
                    extra_info=extra_info,
                )
                score = result if isinstance(result, float) else result.get("score", 0.0)
                
                group_data.append({
                    "original_index": i,
                    "response_str": response_str,
                    "original_score": score,
                    "valid_response_length": valid_response_length.item(),
                })
            
            response_lengths = [item["valid_response_length"] for item in group_data]
            median_length = np.median(response_lengths)

            print(f"[PENALTY DEBUG] Processing group with indices {indices}, median response length: {median_length}")
            
            # Prepare for batched OpenAI calls - SIMPLIFIED VERSION
            responses_to_evaluate_for_batch = [
                (item["response_str"], item["original_score"]) for item in group_data
            ]

            # é€‰æ‹© OpenAI è¯„ä¼°æ–¹æ³• - SIMPLIFIED
            if self.async_openai_manager:
                # âœ… ä½¿ç”¨çœŸæ­£çš„å¼‚æ­¥ I/Oï¼Œé›¶ GPU ç­‰å¾…æ—¶é—´
                logger.info(f"ğŸš€ Using TRUE ASYNC I/O for batch with {len(responses_to_evaluate_for_batch)} responses (zero GPU wait)")
                batched_openai_quality_rewards = self._get_batched_openai_quality_rewards_async(
                    responses_to_evaluate_for_batch
                )
            else:
                # âš ï¸ å›é€€åˆ°åŒæ­¥æ–¹æ³•ï¼ˆæ—  OpenAI è¯„ä¼°æˆ–ç¦ç”¨å¼‚æ­¥ï¼‰
                logger.info(f"âš¡ Using synchronous method for batch with {len(responses_to_evaluate_for_batch)} responses (set use_async_io=True for async)")
                batched_openai_quality_rewards = self._get_batched_openai_quality_rewards_sync(
                    responses_to_evaluate_for_batch
                )

            for idx_in_group, item in enumerate(group_data):
                i = item["original_index"]
                response_str = item["response_str"]
                score = item["original_score"]
                valid_response_length = item["valid_response_length"]
                length = valid_response_length # For clarity in penalty calculation

                reward = score
                
                openai_quality_reward = batched_openai_quality_rewards[idx_in_group]
                reward += openai_quality_reward
                reward_extra_info["openai_quality_reward"].append(openai_quality_reward)
                
                scaled_penalty = 0.0
                print(f"[PENALTY DEBUG] length_penalty_config {self.length_penalty_config}")
                # Apply piecewise length-based penalty
                if self.length_penalty_config and self.length_penalty_config.enable:
                    print(f"[PENALTY DEBUG] Length penalty calculation is ENABLED.")
                    
                    penalty_scale = getattr(self.length_penalty_config, "penalty_scale", 1.0)
                    max_penalty = getattr(self.length_penalty_config, "max_penalty", 1.0)
                    peak_ratio = getattr(self.length_penalty_config, "peak_ratio", 0.3)
                    outer_ratio = getattr(self.length_penalty_config, "outer_ratio", 0.5)
                    print(f"[PENALTY DEBUG] Config: penalty_scale={penalty_scale}, max_penalty={max_penalty}, peak_ratio={peak_ratio}, outer_ratio={outer_ratio}")

                    if outer_ratio <= peak_ratio:
                        raise ValueError("outer_ratio must be greater than peak_ratio in length_penalty_config")

                    penalty_component = 0.0
                    if median_length > 0:
                        linear_start = median_length * (1 - outer_ratio)
                        peak_start = median_length * (1 - peak_ratio)
                        peak_end = median_length * (1 + peak_ratio)
                        linear_end = median_length * (1 + outer_ratio)
                        
                        print(f"[PENALTY DEBUG] length={length}, median_length={median_length}")
                        print(f"[PENALTY DEBUG] No-penalty zone: [{peak_start:.2f}, {peak_end:.2f}]")
                        print(f"[PENALTY DEBUG] Linear penalty zone: [{linear_start:.2f}, {peak_start:.2f}) U ({peak_end:.2f}, {linear_end:.2f}]")

                        if length < linear_start or length > linear_end:
                            penalty_component = max_penalty
                        elif length >= linear_start and length < peak_start:
                            denominator = peak_start - linear_start
                            if denominator > 0:
                                penalty_component = max_penalty * (peak_start - length) / denominator
                        elif length > peak_end and length <= linear_end:
                            denominator = linear_end - peak_end
                            if denominator > 0:
                                penalty_component = max_penalty * (length - peak_end) / denominator
                        
                        print(f"[PENALTY DEBUG] Calculated penalty_component: {penalty_component:.4f}")

                    scaled_penalty = penalty_component * penalty_scale
                    reward -= scaled_penalty
                    reward_extra_info["length_penalty"].append(scaled_penalty)


                print(f"Reward calculated. Total: {reward}, Score: {score}, Length Penalty: {-scaled_penalty}, OpenAI Quality Reward: {openai_quality_reward}")
                reward_tensor[i, int(valid_response_length) - 1] = reward
                reward_extra_info["original_score"].append(score)
                reward_extra_info["response_length"].append(length)
                reward_extra_info["median_length"].append(median_length)

        if return_dict:
            result = {
                "reward_tensor": reward_tensor,
                "reward_extra_info": reward_extra_info,
            }
        else:
            result = reward_tensor
        return result
